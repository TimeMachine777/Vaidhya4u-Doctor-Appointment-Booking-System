{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2uiml_fOTPL_"
      },
      "outputs": [],
      "source": [
        "pip install autocorrect"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyspellchecker"
      ],
      "metadata": {
        "id": "2ij0yshD0KEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T76HsqWHA3yq"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download en_core_web_lg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5rlY1cCTHc_"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from spacy import displacy\n",
        "from autocorrect import Speller\n",
        "from spellchecker import SpellChecker\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "# nlp = spacy.load('en_core_web_lg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08PA3UGua9KB"
      },
      "outputs": [],
      "source": [
        "short_forms1 = {\n",
        "    \"SOB\": \"Shortness of Breath\",\n",
        "    \"CP\": \"Chest Pain\",\n",
        "    \"HA\": \"Headache\",\n",
        "    \"N/V\": \"Nausea/Vomiting\",\n",
        "    \"F/C\": \"Fever/Chills\",\n",
        "    \"D/C\": \"Discharge (from a wound)\",\n",
        "    \"URI\": \"Upper Respiratory Infection\",\n",
        "    \"UTI\": \"Urinary Tract Infection\",\n",
        "    \"BM\": \"Bowel Movement\",\n",
        "    \"DVT\": \"Deep Vein Thrombosis\",\n",
        "    \"HTN\": \"Hypertension\",\n",
        "    \"DM\": \"Diabetes Mellitus\",\n",
        "    \"COPD\": \"Chronic Obstructive Pulmonary Disease\",\n",
        "    \"RA\": \"Rheumatoid Arthritis\",\n",
        "    \"GERD\": \"Gastroesophageal Reflux Disease\",\n",
        "    \"OTC\": \"Over-the-Counter\",\n",
        "    \"Rx\": \"Prescription\",\n",
        "    \"NSAID\": \"Nonsteroidal Anti-Inflammatory Drug\",\n",
        "    \"MRI\": \"Magnetic Resonance Imaging\",\n",
        "    \"CT\": \"Computed Tomography\",\n",
        "    \"CBC\": \"Complete Blood Count\",\n",
        "    \"ECG/EKG\": \"Electrocardiogram\",\n",
        "    \"IV\": \"Intravenous\",\n",
        "    \"PCP\": \"Primary Care Physician\",\n",
        "    \"ER\": \"Emergency Room\",\n",
        "    \"ICU\": \"Intensive Care Unit\",\n",
        "    \"AMA\": \"Against Medical Advice\",\n",
        "    \"NPO\": \"Nothing by Mouth\",\n",
        "    \"meds\": \"medications\",\n",
        "    \"doc\": \"doctor\",\n",
        "    \"xray\": \"X-ray\",\n",
        "    \"mri\": \"magnetic resonance imaging\",\n",
        "    \"er\": \"emergency room\",\n",
        "    \"vet\": \"veterinarian\",\n",
        "    \"gp\": \"general practitioner\",\n",
        "    \"hmd\": \"Help, Medical Emergency!\",\n",
        "    \"sos\" : \"Save Our Souls\",\n",
        "    \"tia\": \"This is an emergency\",\n",
        "    \"code blue\": \"Medical emergency requiring resuscitation\" ,\n",
        "    \"rta\": \"Requesting Treatment ASAP\",\n",
        "    \"ama\": \"Against Medical Advice\",\n",
        "    \"icu\": \"Intensive Care Unit\",\n",
        "    \"dnr\": \"Do Not Resuscitate\",\n",
        "    \"od\": \"Overdose\",\n",
        "    \"ptsd\": \"Post-Traumatic Stress Disorder\",\n",
        "    \"ems\": \"Emergency Medical Services\",\n",
        "    \"apap\": \"As Per Approved Protocol\",\n",
        "    \"iv\": \"Intravenous\",\n",
        "    \"ehr\": \"Electronic Health Record\",\n",
        "    \"rx\": \"Prescription\",\n",
        "    \"npo\": \"Nothing by Mouth\",\n",
        "    \"l&d\": \"Labor and Delivery\",\n",
        "    \"pmh\": \"Past Medical History\",\n",
        "    \"asap\": \"As Soon As Possible\",\n",
        "    \"loc\": \"Loss of Consciousness\",\n",
        "    \"pe\": \"Pulmonary Embolism\",\n",
        "    \"cdc\": \"Centers for Disease Control and Prevention\",\n",
        "    \"bpm\": \"Beats Per Minute\",\n",
        "    \"emt\": \"Emergency Medical Technician\",\n",
        "    \"afib\": \"Atrial Fibrillation\",\n",
        "    \"abo\": \"Already Been Operated\",\n",
        "    \"tbi\": \"Traumatic Brain Injury\",\n",
        "    \"gerd\": \"Gastroesophageal Reflux Disease\",\n",
        "    \"ent\": \"Ear, Nose, and Throat\",\n",
        "    \"ra\": \"Rheumatoid Arthritis\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pO5GmNdF8cvS"
      },
      "outputs": [],
      "source": [
        "#forming final short forms dictionary\n",
        "short_forms = {}\n",
        "for i in short_forms1:\n",
        "  short_forms[i.lower()]=short_forms1[i].lower()\n",
        "# short_forms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ziHj0gFPdBPe"
      },
      "outputs": [],
      "source": [
        "#Storing the short forms in a binary file (pickle file)\n",
        "import pickle\n",
        "f=open(\"short_forms.pkl\",'wb')\n",
        "pickle.dump(short_forms,f)\n",
        "f.flush()\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Myxxv3KOVMPP"
      },
      "outputs": [],
      "source": [
        "def socialmediacheck(word):\n",
        "  if word in short_forms:\n",
        "    return short_forms[word]\n",
        "  else:\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8oZjGs0Trki"
      },
      "outputs": [],
      "source": [
        "def spellchecker(word):\n",
        "  word=word.lower()\n",
        "  # spell = Speller() #autocorrect lib func\n",
        "  spell = SpellChecker() #pyspellchecker lib func\n",
        "  t1=socialmediacheck(word)\n",
        "  if t1!=None:\n",
        "    return t1\n",
        "  else:\n",
        "    return spell.correction(word)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_time(time_str):\n",
        "\n",
        "    # Split the time string by ':' and space to separate hours and minutes\n",
        "    parts = time_str.split(':')\n",
        "    hours, minutes = int(parts[0]), int(parts[1].split()[0])\n",
        "\n",
        "    # Check if it's AM or PM and adjust the hours if necessary\n",
        "    if \"pm\" in time_str.lower() and hours != 12:\n",
        "        hours += 12\n",
        "    elif \"am\" in time_str.lower() and hours == 12:\n",
        "        hours = 0\n",
        "\n",
        "    # Convert hours and minutes to a 4-digit string\n",
        "    time_24hr = f\"{hours:02}{minutes:02}\"\n",
        "\n",
        "    return time_24hr"
      ],
      "metadata": {
        "id": "yX4Iv8_-NQOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWpYDDBelI_P"
      },
      "outputs": [],
      "source": [
        "# text=input(\"Enter input text:\")\n",
        "# text=\"Hello everyonee, myshelf Gaurang Dixit, I'm having nausea. Book me an appointment ASAP on 29th of October at 6pm or else you willl go to jail and Gaurang Dixit will be sad. \"\n",
        "text=\"Hello my name is Gaurang Dixit and I keep sneezing, and my eyes don't quit dripping. \"\n",
        "doc=nlp(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CcOj2C8NYu1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "379699cc-860e-494a-a374-5ede1205157f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Gaurang Dixit'}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "#getting all named entities (built in entites in SpaCy)\n",
        "ent_set=set({})\n",
        "for ent in doc.ents:\n",
        "  if ent.label_ == 'PERSON':\n",
        "    ent_set.add(ent.text)\n",
        "\n",
        "ent_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwdveoPN7hSB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c007fa70-9660-4b56-b21d-d551cf160af0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello:None\n",
            "my:None\n",
            "name:None\n",
            "is:None\n",
            "Gaurang:None\n",
            "Dixit:None\n",
            "and:None\n",
            "I:None\n",
            "keep:None\n",
            "sneezing:None\n",
            ",:None\n",
            "and:None\n",
            "my:None\n",
            "eyes:None\n",
            "do:None\n",
            "n't:None\n",
            "quit:None\n",
            "dripping:None\n",
            ".:None\n"
          ]
        }
      ],
      "source": [
        "#testing for short forms\n",
        "for i in doc:\n",
        "  print(i.text,socialmediacheck(i.text.lower()),sep=\":\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qcxln0mWmNPc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d3d7d14-9e71-40e4-8660-66a31bc16af1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'-1': 'Gaurang Dixit'}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "#replacing all entities with a single word (for easy searching and storing those abbr in a dict for later so as to get back the original string)\n",
        "ent_all={}\n",
        "ct=-1 #key value for entities\n",
        "for i in ent_set:\n",
        "  ent_all[str(ct)]=i\n",
        "  text=text.replace(str(i),str(ct))\n",
        "  ct-=1\n",
        "\n",
        "# text\n",
        "#recreating doc object\n",
        "doc=nlp(text)\n",
        "ent_all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Trj8erIrfBpg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27eef519-1a2b-4263-9ad2-2e1ef3294bd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final text is:  hello my name is Gaurang Dixit and i keep sneezing , and my eyes do not quit dripping . \n"
          ]
        }
      ],
      "source": [
        "#creating final corrected and short form free text sentence\n",
        "text_final=' '\n",
        "for tok in doc:\n",
        "  if tok.text not in ent_all:\n",
        "    text_final+=spellchecker(tok.text)\n",
        "  else:\n",
        "    text_final+=tok.text\n",
        "  text_final+=\" \"\n",
        "\n",
        "for i in ent_all:\n",
        "  text_final=text_final.replace(i,ent_all[i])\n",
        "print(\"Final text is:\",text_final)\n",
        "\n",
        "#---------------------------------------------------------------\n",
        "#auto correct and social media pre-processing done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpBdTPetwSa9"
      },
      "outputs": [],
      "source": [
        "#all the preprocessing of the text\n",
        "def preprocess(text):\n",
        "  doc=nlp(text)\n",
        "\n",
        "  #getting all named entities (built in entites in SpaCy)\n",
        "  ent_set=set({})\n",
        "  for ent in doc.ents:\n",
        "    if ent.label_ in ['PERSON','TIME']:\n",
        "      ent_set.add(ent.text)\n",
        "  # print(\"Ent set:\")\n",
        "  # for i in ent_set:\n",
        "  #   print(i)\n",
        "\n",
        "  #replacing all entities with a single word (for easy searching and storing those abbr in a dict for later so as to get back the original string)\n",
        "  ent_all={}\n",
        "  ct=-1 #key value for entities\n",
        "  for i in ent_set:\n",
        "    ent_all[str(ct)]=i\n",
        "    text=text.replace(str(i),str(ct))\n",
        "    ct-=1\n",
        "  # print(\"Ent all:\")\n",
        "  # for i in ent_all:\n",
        "  #   print(i,\":\",ent_all[i])\n",
        "  # print(\"Text:\",text)\n",
        "  #creating final corrected and short form free text sentence\n",
        "\n",
        "  doc=nlp(text)\n",
        "  text_final=' '\n",
        "  for tok in doc:\n",
        "    if tok.text not in ent_all:\n",
        "      text_final+=spellchecker(tok.text)\n",
        "    else:\n",
        "      text_final+=tok.text\n",
        "    text_final+=\" \"\n",
        "\n",
        "  for i in ent_all:\n",
        "    text_final=text_final.replace(i,ent_all[i])\n",
        "  # print(\"Final text is:\",text_final)\n",
        "  return text_final\n",
        "#---------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nvj1zRPj9qeN"
      },
      "outputs": [],
      "source": [
        "#Now classification task begins"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P366FaF3HwV4"
      },
      "outputs": [],
      "source": [
        "#Getting text and label from dataset\n",
        "df=pd.read_csv('Symptom2Disease.csv')\n",
        "labels = df['label'].tolist()\n",
        "text = df['text'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels"
      ],
      "metadata": {
        "id": "ppzH6cJOd7gX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pre-processing dataset text\n",
        "text_final = []\n",
        "for i in text:\n",
        "  doc1 = nlp(i)\n",
        "  sent= [tok.text.lower() for tok in doc1 if not tok.is_punct and not tok.is_space]\n",
        "  sent_final= ' '.join(token for token in sent if token not in nlp.Defaults.stop_words)\n",
        "  text_final.append(sent_final)"
      ],
      "metadata": {
        "id": "LGOtLsNpGFo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-rbJBA4dcUF"
      },
      "outputs": [],
      "source": [
        "#label substituting (pre-preprocessing dataset labels)\n",
        "label_to_no={}\n",
        "no_to_label={}\n",
        "ct=0\n",
        "for i in range(len(labels)):\n",
        "  labels[i]=labels[i].lower()\n",
        "  if labels[i] not in label_to_no:\n",
        "    label_to_no[labels[i]]=ct\n",
        "    no_to_label[ct]=labels[i]\n",
        "    labels[i]=ct\n",
        "    ct+=1\n",
        "  else:\n",
        "    labels[i]=label_to_no[labels[i]]\n",
        "\n",
        "labels=np.array([i for i in labels])\n",
        "no_to_label\n",
        "# label_to_no"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNPbkPW1R6tO"
      },
      "outputs": [],
      "source": [
        "#experimenting------------------------\n",
        "\n",
        "#here using vstack is the same as creating an array by using append function, but we use it to maintain standards and remove any confusion\n",
        "#becauase there is another function hstack which joins array horizontally, i.e. adding row1 of mat1 to row1 of mat2, then row2 of mat1 to row2 of mat2,....\n",
        "#whereas in vstack it is simple append like first all rows of mat1, then all rows of mat2 one by one as happens in append (see diagram in net to understand better)\n",
        "feature_matrix = np.vstack([token.vector for token in doc])\n",
        "# feature_matrix= [list(i.vector[:3]) for i in doc]\n",
        "feature_matrix\n",
        "\n",
        "#experimenting------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_8Leb3CXjvW"
      },
      "outputs": [],
      "source": [
        "#creating feature matrix for training data (using word vector of spacy)\n",
        "feature_matrix = []\n",
        "\n",
        "# Loop through the text data\n",
        "for i in text_final:\n",
        "    # Process the text with spaCy\n",
        "    doc1 = nlp(i)\n",
        "    feature_matrix.append(doc1.vector)\n",
        "\n",
        "# Convert the list of feature matrices to a single numpy array\n",
        "feature_matrix = np.array(feature_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#creating feature matrix for training data (using tf-idf of sk-learn)\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "feature_matrix = vectorizer.fit_transform(text_final)\n",
        "# feature_matrix = tfidf_matrix.toarray()\n",
        "# tfidf_array"
      ],
      "metadata": {
        "id": "gK30dm0R8Wkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNmJLitrQPfV"
      },
      "outputs": [],
      "source": [
        "#dividing data into train and test data\n",
        "text_train, text_test, labels_train, labels_test = train_test_split(feature_matrix, labels, test_size=0.2, random_state=41)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hq6IZWJeQ76P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c5947b4-4682-4f39-cca6-4ae49409243c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 : 40\n",
            "1 : 42\n",
            "2 : 43\n",
            "3 : 41\n",
            "4 : 41\n",
            "5 : 45\n",
            "6 : 38\n",
            "7 : 41\n",
            "8 : 43\n",
            "9 : 43\n",
            "10 : 35\n",
            "11 : 43\n",
            "12 : 40\n",
            "13 : 45\n",
            "14 : 38\n",
            "15 : 33\n",
            "16 : 39\n",
            "17 : 44\n",
            "18 : 36\n",
            "19 : 40\n",
            "20 : 37\n",
            "21 : 41\n",
            "22 : 38\n",
            "23 : 34\n"
          ]
        }
      ],
      "source": [
        "#experimenting-------------\n",
        "x=list(labels_train)\n",
        "for i in range(0,24):\n",
        "  print(i,\":\",x.count(i))\n",
        "#--------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mgj-bKb2cuxR"
      },
      "outputs": [],
      "source": [
        "#training model\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "# model=LogisticRegression()\n",
        "# model = MultinomialNB()\n",
        "# model = SVC(kernel='linear')\n",
        "# model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model = MLPClassifier(hidden_layer_sizes=(100, ), max_iter=1000, random_state=42, activation='tanh')\n",
        "\n",
        "model.fit(text_train,labels_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMUX-yKWksms"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "label_predict=model.predict(text_test)\n",
        "report_dict=classification_report(labels_test,label_predict)\n",
        "print(report_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RMKeZCT9s4L",
        "outputId": "afe03dd1-8e91-4648-f607-81e2c9639611"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final text is:  hello my name is Gaurang Dixit and i have loose motion and pain in the stomach . book me an appointment at 11:40 pm \n"
          ]
        }
      ],
      "source": [
        "# nlp_l=spacy.load('en_core_web_lg')\n",
        "user_text=\"Hello my name is Gaurang Dixit and I have loose motion and pain in the stomach. Book me an appointment at 11:40 pm\"\n",
        "# user_text=\"Hi myself Vishwak, I am having cough,cold and high fever.\"\n",
        "user_text_final=preprocess(user_text)\n",
        "print(\"Final text is:\",user_text_final)\n",
        "doc=nlp(user_text_final)\n",
        "tokens= [tok.text.lower() for tok in doc if not tok.is_punct and not tok.is_space]\n",
        "tokens= [token for token in tokens if token not in nlp.Defaults.stop_words]\n",
        "doc=nlp(' '.join(i for i in tokens)) #final nlp object\n",
        "# tokens\n",
        "#for now not doing lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZaeTHmB_J_k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65a55bc1-c580-46cd-dfd0-8d0aa365eb4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello gaurang dixit loose motion pain stomach book appointment 11:40 pm\n"
          ]
        }
      ],
      "source": [
        "print(doc.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTVcrS1-lOn7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "2aba0b15-d681-4378-e564-ba260163a904"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'typhoid'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "#testing on the given text\n",
        "# guess=model.predict(np.array([doc.vector,doc.vector]))\n",
        "guess=model.predict(vectorizer.transform([doc.text]))\n",
        "no_to_label[guess[0]]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc=nlp(user_text_final)\n",
        "for i in doc.ents:\n",
        "  if i.label_=='TIME':\n",
        "    print(i.text,convert_time(i.text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7-auf9_JRzC",
        "outputId": "8c03874b-59fd-4aaa-d88d-0b20fb9bc396"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11:40 pm 2340\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_to_no"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tS9Gz800e6EG",
        "outputId": "1a3ec406-6292-4613-b895-52cd054facbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'psoriasis': 0,\n",
              " 'varicose veins': 1,\n",
              " 'typhoid': 2,\n",
              " 'chicken pox': 3,\n",
              " 'impetigo': 4,\n",
              " 'dengue': 5,\n",
              " 'fungal infection': 6,\n",
              " 'common cold': 7,\n",
              " 'pneumonia': 8,\n",
              " 'dimorphic hemorrhoids': 9,\n",
              " 'arthritis': 10,\n",
              " 'acne': 11,\n",
              " 'bronchial asthma': 12,\n",
              " 'hypertension': 13,\n",
              " 'migraine': 14,\n",
              " 'cervical spondylosis': 15,\n",
              " 'jaundice': 16,\n",
              " 'malaria': 17,\n",
              " 'urinary tract infection': 18,\n",
              " 'allergy': 19,\n",
              " 'gastroesophageal reflux disease': 20,\n",
              " 'drug reaction': 21,\n",
              " 'peptic ulcer disease': 22,\n",
              " 'diabetes': 23}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Dictionary 1 disease to specialisation\n",
        "disease_specialization = {\n",
        "    'psoriasis': 'Dermatology',\n",
        "    'impetigo': 'Dermatology',\n",
        "    'pneumonia': 'Pulmonology',\n",
        "    'dengue': 'Virology',\n",
        "    'common cold': 'Virology',\n",
        "    'varicose veins':'Dermatology',\n",
        "    'typhoid':'Gastroenterology',\n",
        "    'chicken pox':'Dermatology',\n",
        "    'fungal infection':'General',\n",
        "    'dimorphic hemorrhoids':'Gastroenterology',\n",
        "    'arthritis':'Orthology',\n",
        "    'acne':'Dermatology',\n",
        "    'bronchial asthma':'Pulmonology',\n",
        "    'hypertension':'General',\n",
        "    'migraine':'General',\n",
        "    'cervical spondylosis':'Orthology',\n",
        "    'jaundice':'Gastroenterology',\n",
        "    'malaria':'Virology',\n",
        "    'urinary tract infection':'General',\n",
        "    'allergy':'General',\n",
        "    'gastroesophageal reflux disease':'Gastroenterology',\n",
        "    'drug reaction':'General',\n",
        "    'peptic ulcer disease':'Gastroenterology',\n",
        "    'diabetes':'General',\n",
        "}\n",
        "\n",
        "\n",
        "specialization_doctors = {\n",
        "    'Dermatology': {\n",
        "        '0': 'Dr.Ramesh',\n",
        "        '1': 'Dr.Michael',\n",
        "    },\n",
        "    'Pulmonology': {\n",
        "        '2': 'Dr.Chitambaram',\n",
        "        '3': 'Dr.Richard',\n",
        "    },\n",
        "    'Virology': {\n",
        "        '4': 'Dr.Prateek Kumar',\n",
        "        '5': 'Dr.Rory',\n",
        "    },\n",
        "    'Gastroenterology': {\n",
        "        '6': 'Dr.Mishra',\n",
        "        '7': 'Dr.Stephen',\n",
        "    },\n",
        "    'Orthology': {\n",
        "        '8': 'Dr.Sandeep',\n",
        "        '9': 'Dr.Robert',\n",
        "    },\n",
        "    'General': {\n",
        "        '10': 'Dr.Vinod',\n",
        "        '11': 'Dr.Jeffery',\n",
        "    },\n",
        "}\n",
        "\n",
        "doctor_availability = {\n",
        "    '0': ['1130'],\n",
        "    '1': ['1000', '1530'],\n",
        "    '2': [ '1200'],\n",
        "    '3': [ '1130'],\n",
        "    '4': ['0800', '1030', '1600'],\n",
        "    '5': ['0945', '1315'],\n",
        "    '6': ['1000', '1115'],\n",
        "    '7': ['0915', '1115', '1315'],\n",
        "    '8': ['1000', '1115'],\n",
        "    '9': ['1030', '1800'],\n",
        "    '10': ['0845', '1100'],\n",
        "    '11': ['1145', '1545'],\n",
        "}\n"
      ],
      "metadata": {
        "id": "PyzR-7P7K9Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "f=open(\"Disease_DB.pkl\",'wb')\n",
        "pickle.dump(disease_specialization,f)\n",
        "pickle.dump(specialization_doctors,f)\n",
        "pickle.dump(doctor_availability,f)\n",
        "f.flush()\n",
        "f.close()"
      ],
      "metadata": {
        "id": "lfxDOoUKl29d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f=open(\"Disease_DB.pkl\",'rb+')"
      ],
      "metadata": {
        "id": "TyiSaSkemhj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#experimenting\n",
        "# lst=[]\n",
        "# f.seek(0,0)\n",
        "# try:\n",
        "#   while True:\n",
        "#     a=pickle.load(f)\n",
        "#     lst.append(a)\n",
        "# except:\n",
        "#   print(\"hello\")\n",
        "\n",
        "# for i in lst:\n",
        "#   print(type(i))"
      ],
      "metadata": {
        "id": "F7yDisKwsSX9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}